{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from time import sleep\n",
    "import random\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import urllib.request\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label - 구하고자 하는 카테고리별 고정 값 지정\n",
    "labels = {\n",
    "    \"경제/정책\": \"economic-policy\",\n",
    "    \"금융\": \"finance\",\n",
    "    \"증시\": \"stock-market\",\n",
    "    \"부동산\": \"real-estate\",\n",
    "    \"소비자\": \"consumer\",\n",
    "    \"국제경제\": \"international-economy\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-20 15:29:37.427236\n",
      "20230820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# date\n",
    "dt = datetime.datetime.now()  # 현재 날짜\n",
    "pages = int(input(\"몇 페이지? > \"))  # 몇 페이지를 부를지\n",
    "print(dt)\n",
    "date = dt.strftime(\"%Y%m%d\")  # str로 변환\n",
    "print(date)\n",
    "\n",
    "pages_list = list()\n",
    "for page in range(1, pages+1):  # 페이지 추가\n",
    "    pages_list.append(page)\n",
    "len(pages_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title과 content 반환\n",
    "def make_head_content(url_list, day, label):\n",
    "    sub_df = pd.DataFrame()\n",
    "\n",
    "    for url in url_list:\n",
    "        r2 = requests.get(url) # 요청 받기\n",
    "        soup2 = BeautifulSoup(r2.text, \"html.parser\")  # html 정보를 txt로 받아오기\n",
    "\n",
    "        # 불필요 기호 제거\n",
    "        pattern1 = '<[^>]*>|\\t|\\n|<b>|</b>|&apos;' # 불필요한 기호 제거\n",
    "\n",
    "        # title\n",
    "        title = soup2.select_one(\"header.title-article01 > h1.tit\") # 기사제목 불러오기\n",
    "        if title == None:\n",
    "            title = None\n",
    "        else:\n",
    "            title = title.get_text() # 타이틀 텍스트로 받기\n",
    "\n",
    "        article = soup2.select(\"div.scroller01 > article.story-news.article > p\") # 기사본문 받기\n",
    "        article = \"\".join([text.get_text() for text in article]) # 기사 본문들 text로 연결\n",
    "\n",
    "        if article == None: \n",
    "            article = None\n",
    "        else:\n",
    "            article = re.sub(pattern=pattern1, repl='', string=str(article)) # 기사에서 특수부호 제거\n",
    "            \n",
    "        if len(article) < 300: # 기사 본문이 300자 이하인 기사는 제거\n",
    "            title = None\n",
    "            content = None\n",
    "        for tag in [\"[표]\", \"[인사]\", \"[코스피]\", \"[코스닥]\", \"[연합뉴스 이 시각 헤드라인]\"]: # 내용이 아닌 알림과 관련된 기사 제거\n",
    "            if tag in article:\n",
    "                title=None\n",
    "                content = None\n",
    "        else:        \n",
    "            # 전체 지문의 20%만 사용\n",
    "            limit = int(round(len(article)*0.2, 0))\n",
    "            content = article[:limit]+\" ...중략...\"\n",
    "\n",
    "        sub = pd.DataFrame({\"title\": [title], \"content\": [content], \"date\": [day], \"label\": [label]}) # 데이터 추가\n",
    "        sub_df = pd.concat([sub_df, sub], axis=0)\n",
    "\n",
    "        sleep(random.uniform(1, 1.5))  # 막히지 않으려 약간의 변동 주기\n",
    "\n",
    "    return sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [10:45<00:00, 215.17s/it]\n"
     ]
    }
   ],
   "source": [
    "# 짧은 기간 사용 - 하루에 한 페이지\n",
    "news_df = pd.DataFrame()\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(\"--headless\")  # 화면창 백그라운드\n",
    "chrome_options.add_argument(\"--disable-gpu\")  # 불필요한 gpu 사용 X\n",
    "chrome_options.add_argument(\"--no-sandbox\") # 보안기능 강화 해제\n",
    "chrome_options.add_argument(\"--disable-setuid-sandbox\") # 보안기능 비활성화\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\") # 메모리 사용량 감소\n",
    "#cnt = 1\n",
    "\n",
    "def main(pages_list, news_df):\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options) # 크롬 드라이버 사용\n",
    "\n",
    "    for page in tqdm(pages_list):\n",
    "        for label in labels:\n",
    "            driver.get(f\"https://www.yna.co.kr/economy/{labels[label]}/{page}\") # url을 통한 접근\n",
    "            element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div.paging.paging-type01\"))) ## 해당 요소가 나타날 때까지 대기\n",
    "\n",
    "            # html 정보를 txt로 받아오기\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "            # 각 기사별 링크 가져오기\n",
    "            cnt = soup.select(\"div.list-type038 > ul.list > li > div.item-box01 > div.news-con > a\")\n",
    "\n",
    "            # url_list 생성\n",
    "            url_list = [f\"https:{link.attrs['href']}\" for link in cnt]\n",
    "            \n",
    "            # 제목과 내용 반환\n",
    "            sub_df = make_head_content(url_list, date, label)\n",
    "            news_df = pd.concat([news_df, sub_df], axis=0)\n",
    "\n",
    "    driver.quit()\n",
    "    return news_df\n",
    "\n",
    "\n",
    "news_df = main(pages_list, news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(398, 4)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = news_df.dropna(axis=0) # null값 제거\n",
    "news_df.reset_index(drop=True, inplace=True) # 인덱스 초기화\n",
    "\n",
    "# id 생성\n",
    "news_df[\"news_id\"] = [f\"news_{i}\" for i in range(1, news_df.shape[0]+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv로 저장\n",
    "news_df.to_csv(os.path.join(os.getcwd(), \"data\", \"news_db.csv\"), encoding=\"utf-8\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kb_ai",
   "language": "python",
   "name": "kb_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
