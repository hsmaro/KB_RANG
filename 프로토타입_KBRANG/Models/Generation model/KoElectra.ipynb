{"cells":[{"cell_type":"markdown","metadata":{},"source":["## KoELECTRA"]},{"cell_type":"markdown","metadata":{},"source":["### 필요한 모델 및 라이브러리 불러오기"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4707,"status":"ok","timestamp":1692466662163,"user":{"displayName":"김동하","userId":"06430207791946595988"},"user_tz":-540},"id":"SXiVVaTpScou","outputId":"a4fb054b-31d7-4275-f80c-ecb7194c0d0b"},"outputs":[],"source":["!pip install torch transformers # transformers download"]},{"cell_type":"markdown","metadata":{},"source":["### Library"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PpZeeMFx9EHy"},"outputs":[],"source":["# Library\n","import os\n","\n","import torch\n","from transformers import ElectraForSequenceClassification, AdamW\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import Adam\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score\n","from transformers import AutoModel, AutoTokenizer\n","from transformers import ElectraForSequenceClassification, ElectraTokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yhZ2Jx6R38bP"},"outputs":[],"source":["# Load Tokenizer and Model\n","koelectra_model_path = \"/content/drive/MyDrive/kb_chs/data/koelectra-small-v3-discriminator\"  # Update the path here # 가중치가 있는 경로\n","model = AutoModel.from_pretrained(koelectra_model_path) # 사전학습 모델  불러오기\n","tokenizer = AutoTokenizer.from_pretrained(koelectra_model_path) # 사전학습 모델 불러오기"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":438},"executionInfo":{"elapsed":2768,"status":"error","timestamp":1692467351495,"user":{"displayName":"김동하","userId":"06430207791946595988"},"user_tz":-540},"id":"--lhb75t-R4_","outputId":"14f36319-708c-4d2b-8fdd-1df8f2f9f193"},"outputs":[],"source":["# Load Dataset\n","df = pd.read_csv('/content/drive/MyDrive/kb_chs/data/aihub_ox.csv')\n","\n","# Custom Dataset\n","class QADataset(Dataset):\n","    def __init__(self, df, tokenizer):\n","        self.df = df # 문제 데이터 파일과 토큰화된 데이터\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        context = self.df['context'].iloc[idx] # 질문의 바탕이 되는 내용\n","        question = self.df['question'].iloc[idx] # 질문\n","        answer = self.df['answer'].iloc[idx] # 정답\n","        inputs = self.tokenizer.encode_plus(question, context, max_length=512, padding='max_length', truncation=True, return_tensors='pt', return_token_type_ids=False)\n","        return {'inputs': inputs, 'answer': answer}\n","\n","# Split the dataframe into train and validation data\n","train_df, val_df = train_test_split(df, test_size=0.2, random_state=42) # 데이터 분리\n","\n","# Initialize Datasets and Dataloaders for train and validation set\n","train_dataset = QADataset(train_df, tokenizer) # 훈련 데이터\n","val_dataset = QADataset(val_df, tokenizer) # 검정 데이터\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True) # 배치 변경\n","val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)  # No need to shuffle validation data\n","\n","# Initialize model (KoElectra 사용)\n","model = ElectraForSequenceClassification.from_pretrained(koelectra_model_path, num_labels=2)\n","model = model.to('cuda')\n","\n","# Initialize optimizer\n","optimizer = AdamW(model.parameters(), lr=1e-5)\n","\n","best_loss = 9999999\n","max_patience = 5\n","num_patience = 0\n","\n","# Model Training\n","for epoch in range(50):\n","    model.train()\n","    train_loss_list = []\n","    for batch in train_dataloader:\n","        inputs = {k: v.squeeze(1).to('cuda') for k, v in batch['inputs'].items()}\n","        input_ids = inputs['input_ids'] # 입력문장의 토큰 ID\n","        attention_mask = inputs['attention_mask'] # 어텐션 마스트\n","        labels = batch['answer'].to('cuda') #\n","\n","        optimizer.zero_grad() # 그레디언트 초기화\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss # 손실계산\n","        loss.backward() # 역전파\n","        optimizer.step() # 가중치 업데이트\n","        train_loss_list.append(loss.item())\n","\n","    model.eval()\n","    with torch.no_grad():\n","        total = 0\n","        correct = 0\n","        val_loss_list = []\n","        y_true = []  # 실제 정답값 저장\n","        y_pred = []  # 예측값 저장\n","        for batch in val_dataloader:\n","            inputs = {k: v.squeeze(1).to('cuda') for k, v in batch['inputs'].items()} # 입력 데이터\n","            labels = torch.stack([torch.tensor(int(a)).to('cuda') for a in batch['answer']]) # 정답값\n","            outputs = model(**inputs, labels=labels) # 예측값\n","            loss = outputs.loss\n","            val_loss_list.append(loss.item())\n","\n","            _, predicted = torch.max(outputs.logits, 1) # 예측값 조정\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item() # 정답률\n","            y_true.extend(labels.cpu().numpy()) # 실제값 저장\n","            y_pred.extend(predicted.cpu().numpy()) # 예측값 저장\n","\n","    # y_true와 y_pred를 사용하여 f1 스코어 계산\n","    f1 = f1_score(y_true, y_pred, average='weighted')\n","\n","    print(f'train_loss: {np.mean(train_loss_list):.5f} val_loss: {np.mean(val_loss_list):.5f}') # train과 valid의 loss값\n","    print(f\"val_Accuracy: {(100 * correct / total):.2f} val_F1 Score: {f1:.2f}\") # val 데이터의 정확도와 f1-score\n","\n","    val_loss = np.mean(val_loss_list)\n","    if best_loss > val_loss: # loss 최소화\n","        print(\"Save new model on epoch: %d\" % (epoch + 1))\n","        best_loss = val_loss\n","        best_model = model\n","        print('Model Saved')\n","        num_patience = 0\n","\n","    else:\n","        num_patience += 1\n","\n","    if num_patience >= max_patience: # Early Stop\n","        print(f\"Early Stopped after epoch {epoch+1}\")\n","        break\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1eWafjOJPMLr"},"source":["## Inference\n","- koelectra"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8655,"status":"ok","timestamp":1692467240437,"user":{"displayName":"김동하","userId":"06430207791946595988"},"user_tz":-540},"id":"_etZtTbwQL8i","outputId":"57a58f24-b7cf-48d0-f33e-9cd77cbb6622"},"outputs":[],"source":["# Evaluation\n","model.eval()\n","with torch.no_grad():\n","    total = 0\n","    correct = 0\n","    y_true = []  # 실제 정답값 저장\n","    y_pred = []  # 예측값 저장\n","    for batch in val_dataloader:\n","        inputs = {k: v.squeeze(1).to('cuda') for k, v in batch['inputs'].items()} # 입력 데이터\n","        labels = torch.stack([torch.tensor(int(a)).to('cuda') for a in batch['answer']]) # 정답값\n","        outputs = model(**inputs) # 예측값\n","        _, predicted = torch.max(outputs.logits, 1) # 예측값 조정\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item() # 정답률\n","        y_true.extend(labels.cpu().numpy()) # 실제값 저장\n","        y_pred.extend(predicted.cpu().numpy()) # 예측값 저장\n","\n","# y_true와 y_pred를 사용하여 f1 스코어 계산\n","f1 = f1_score(y_true, y_pred, average='weighted')\n","\n","print('Accuracy: %d %%' % (100 * correct / total))\n","print('F1 Score:', f1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FJtkjmiKQLxd"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOXtdT8cqtYFX3mSi25YkXd","machine_shape":"hm","mount_file_id":"1qxy1E5tZjHVOICciHW5JjaNAaQl3BxaJ","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
