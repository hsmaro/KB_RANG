{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Kobert"]},{"cell_type":"markdown","metadata":{},"source":["### 필요한 라이브러리 설치\n","- https://huggingface.co/skt/kobert-base-v1\n","- 모델 및 가중치 다운로드 필요"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4947,"status":"ok","timestamp":1692468345050,"user":{"displayName":"김동하","userId":"06430207791946595988"},"user_tz":-540},"id":"M8m_5kZaKUEy","outputId":"5f9ecbe2-d002-4c8f-f60a-75cd969b3fcb"},"outputs":[],"source":["!pip install torch transformers # transformers download"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5280,"status":"ok","timestamp":1692468350324,"user":{"displayName":"김동하","userId":"06430207791946595988"},"user_tz":-540},"id":"sdOuChSq84l9","outputId":"abffb1c2-1156-46ef-8e7d-25c43b4f4cea"},"outputs":[],"source":["!pip install sentencepiece"]},{"cell_type":"markdown","metadata":{},"source":["### Library"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6706,"status":"ok","timestamp":1692468359477,"user":{"displayName":"김동하","userId":"06430207791946595988"},"user_tz":-540},"id":"lGaqMLKPMHVn"},"outputs":[],"source":["# sentencepiece 설치필요\n","import os\n","from transformers import AutoModel, AutoTokenizer\n","from transformers import BertForSequenceClassification, BertTokenizer\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","from torch.optim import Adam\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score\n","import numpy as np"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":1467,"status":"ok","timestamp":1692468377297,"user":{"displayName":"김동하","userId":"06430207791946595988"},"user_tz":-540},"id":"ToGumQ3XNNWi"},"outputs":[],"source":["# Load Tokenizer and Model\n","kobert_model_path = \"/content/drive/MyDrive/kb_chs/data/kobert-base-v1\"  # Update the path here # 가중치가 있는 경로\n","model = AutoModel.from_pretrained(kobert_model_path) # 사전학습 모델  불러오기\n","tokenizer = AutoTokenizer.from_pretrained(kobert_model_path, use_fast=True) # 사전학습 모델 불러오기"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":673704,"status":"ok","timestamp":1692469094322,"user":{"displayName":"김동하","userId":"06430207791946595988"},"user_tz":-540},"id":"5Cay8LUjNQy_","outputId":"2cad9bc4-2810-4d47-b082-8e3b30bae996"},"outputs":[],"source":["# Load Dataset\n","df = pd.read_csv('/content/drive/MyDrive/kb_chs/data/aihub_ox.csv')\n","\n","# Custom Dataset\n","class QADataset(Dataset):\n","    def __init__(self, df, tokenizer):\n","        self.df = df # 문제 데이터 파일과 토큰화된 데이터\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        context = self.df['context'].iloc[idx] # 질문의 바탕이 되는 내용\n","        question = self.df['question'].iloc[idx] # 질문\n","        answer = self.df['answer'].iloc[idx] # 정답\n","        inputs = self.tokenizer.encode_plus(question, context, max_length=512, padding='max_length', truncation=True, return_tensors='pt', return_token_type_ids=False)\n","        return {'inputs': inputs, 'answer': answer}\n","\n","# Split the dataframe into train and validation data\n","train, test_df = train_test_split(df, test_size=0.2, random_state=42) # 데이터 분리\n","train_df, val_df = train_test_split(train, test_size=0.2, random_state=42) # 데이터 분리\n","\n","# Initialize Datasets and Dataloaders for train and validation set\n","train_dataset = QADataset(train_df, tokenizer) # 훈련 데이터\n","val_dataset = QADataset(val_df, tokenizer) # 검정 데이터\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True) # 배치 변경\n","val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)  # No need to shuffle validation data\n","\n","# Initialize model (KoBERT 사용)\n","model = BertForSequenceClassification.from_pretrained(kobert_model_path, num_labels=2)  # 2개의 라벨 분류\n","model = model.to('cuda')  # GPU 사용 설정\n","\n","# Initialize optimizer\n","# 옵티마이저 초기화\n","optimizer = Adam(model.parameters(), lr=1e-5)  # Adam 옵티마이저, 학습률 1e-5 설정\n","\n","# 설정값\n","best_loss = 9999999\n","max_patience = 5\n","num_patience = 0\n","\n","# Model Training\n","# 모델 학습 과정\n","for epoch in range(50):  # 50번의 에포크 동안 학습\n","    model.train()\n","    train_loss_list = []\n","    for batch in train_dataloader:  # 훈련 데이터 배치 반복\n","\n","        inputs = {k: v.squeeze(1).to('cuda') for k, v in batch['inputs'].items()}  # GPU 사용 설정하며 입력 데이터 배치 준비\n","        input_ids = inputs['input_ids']  # 입력 문장의 토큰 ID\n","        attention_mask = inputs['attention_mask']  # 어텐션 마스크\n","        labels = batch['answer'].to('cuda')  # 정답 레이블을 GPU로 이동\n","\n","\n","        # 모델 순전파\n","        optimizer.zero_grad()  # 그래디언트 초기화\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss  # 손실 계산\n","\n","\n","        # 역전파 및 가중치 업데이트\n","        loss.backward()  # 역전파 수행\n","        optimizer.step()  # 가중치 업데이트\n","        train_loss_list.append(loss.item())\n","\n","\n","    model.eval()\n","    with torch.no_grad():\n","        total = 0\n","        correct = 0\n","        val_loss_list = []\n","        y_true = []  # 실제 정답값 저장\n","        y_pred = []  # 예측값 저장\n","        for batch in val_dataloader:\n","            inputs = {k: v.squeeze(1).to('cuda') for k, v in batch['inputs'].items()} # 입력 데이터\n","            labels = torch.stack([torch.tensor(int(a)).to('cuda') for a in batch['answer']]) # 정답값\n","            outputs = model(**inputs, labels=labels) # 예측값\n","            loss = outputs.loss\n","            val_loss_list.append(loss.item())\n","\n","            _, predicted = torch.max(outputs.logits, 1) # 예측값 조정\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item() # 정답률\n","            y_true.extend(labels.cpu().numpy()) # 실제값 저장\n","            y_pred.extend(predicted.cpu().numpy()) # 예측값 저장\n","\n","    # y_true와 y_pred를 사용하여 f1 스코어 계산\n","    f1 = f1_score(y_true, y_pred, average='weighted')\n","\n","    print(f'train_loss: {np.mean(train_loss_list):.5f} val_loss: {np.mean(val_loss_list):.5f}') # train과 valid의 loss값\n","    print(f\"val_Accuracy: {(100 * correct / total):.2f} val_F1 Score: {f1:.2f}\") # val 데이터의 정확도와 f1-score\n","\n","    val_loss = np.mean(val_loss_list)\n","    if best_loss > val_loss: # loss 최소화\n","        print(\"Save new model on epoch: %d\" % (epoch + 1))\n","        best_loss = val_loss\n","        best_model = model\n","        print('Model Saved')\n","\n","    else:\n","        num_patience += 1\n","\n","    if num_patience >= max_patience: # Early Stop\n","        print(f\"Early Stopped after epoch {epoch+1}\")\n","        break\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yw67wg9jFKNp"},"outputs":[],"source":["test_dataset = QADataset(test_df, tokenizer) # 검정 데이터\n","test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"]},{"cell_type":"markdown","metadata":{},"source":["## Inference\n","- kobert"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25180,"status":"ok","timestamp":1692469136907,"user":{"displayName":"김동하","userId":"06430207791946595988"},"user_tz":-540},"id":"EQGIDcgSRhb8","outputId":"b8dffacb-a391-4128-9897-c7df5fa11fb0"},"outputs":[],"source":["# Evaluation\n","model.eval()\n","with torch.no_grad():\n","    total = 0\n","    correct = 0\n","    y_true = []  # 실제 정답값 저장\n","    y_pred = []  # 예측값 저장\n","    for batch in test_dataloader:\n","        inputs = {k: v.squeeze(1).to('cuda') for k, v in batch['inputs'].items()} # 입력 데이터\n","        labels = torch.stack([torch.tensor(int(a)).to('cuda') for a in batch['answer']]) # 정답값\n","        outputs = model(**inputs) # 예측값\n","        _, predicted = torch.max(outputs.logits, 1) # 예측값 조정\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item() # 정답률\n","        y_true.extend(labels.cpu().numpy()) # 실제값 저장\n","        y_pred.extend(predicted.cpu().numpy()) # 예측값 저장\n","\n","# y_true와 y_pred를 사용하여 f1 스코어 계산\n","f1 = f1_score(y_true, y_pred, average='weighted')\n","\n","print('Accuracy: %d %%' % (100 * correct / total)) #\n","print('F1 Score:', f1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GZIKsU6_BSob"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMz55dN+YHhQBUHCj7gJ453","gpuType":"A100","machine_shape":"hm","mount_file_id":"1ilA30qUAJJqU8k2ReBsMvLmUk3MZCN6T","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
